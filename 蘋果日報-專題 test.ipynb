{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 步驟一、抓取某一天的所有新聞連結-寫到同一份text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import os\n",
    "news_url=[]\n",
    "page_url = 'http://www.appledaily.com.tw/appledaily/archive/20160310'\n",
    "main_url ='http://www.appledaily.com.tw/appledaily'\n",
    "res = requests.get(page_url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "#包含所有種類的div\n",
    "div = soup.select('.abdominis.clearmen')[0]\n",
    "#抓取所有連結\n",
    "news_url=[]\n",
    "lis = div.select('li')\n",
    "for li in lis:\n",
    "    href = li.select('a')[0]['href']\n",
    "    #news_url.append(urljoin(main_url,'/'.join(href.split('/')[:6])))  #取的新聞連結完整網址\n",
    "    news_url.append(urljoin(main_url,href)) #urljoin會自動合併兩個網址\n",
    "    f = open('C:/Users/BIG DATA/Desktop/Apple_news/20160310.text','w')\n",
    "    for i in news_url:\n",
    "        f.write(i.encode('utf-8')+'\\n') #prettify()先把檔案轉unicode，再用encode('utf-8')編碼\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟二、爬某個新聞的內容，並印到一份txt中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-42-9712db092aaf>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-9712db092aaf>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    with f = open('C:/Users/BIG DATA/Desktop/Apple_news/{}_{}.text'.format(file_date,file_num),'w')\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#爬某個新聞的內文\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string \n",
    "news_all = []\n",
    "url = 'http://www.appledaily.com.tw/appledaily/article/headline/20160310/37102656/%E5%A5%B3%E5%A4%A7%E7%94%9F%E4%BC%B43%E7%94%B7%E9%9B%A2%E5%A5%87%E7%87%92%E7%82%AD%E4%BA%A1'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text)\n",
    "file_date,file_num = url.split('/')[6],url.split('/')[7]  #替檔案取名子\n",
    "\n",
    "\n",
    "#開啟檔案\n",
    "f = open('C:/Users/BIG DATA/Desktop/Apple_news/{}_{}.text'.format(file_date,file_num),'w')\n",
    "#標題\n",
    "title1 = soup.select('#h1')[0].text\n",
    "title2 = soup.select('#h2')[0].text\n",
    "print title1,title2\n",
    "\n",
    "f.write(title1.encode('utf-8')+'\\n')\n",
    "f.write(title2.encode('utf-8')+'\\n')\n",
    "\n",
    "#副標題      \n",
    "h2s = div.select('h2')            \n",
    "for h2 in h2s:\n",
    "    print h2.text\n",
    "    sub_titile = h2.text\n",
    "    f.write(sub_titile.encode('utf-8')+'\\n')\n",
    "    \n",
    "#內文\n",
    "div= soup.select('.articulum')[0]   #先把所有內文抓到\n",
    "ps = div.select('p')                #再把裡面所有的p標籤抓到\n",
    "for p in ps:                        #把每個p標籤一個一個抓出來印\n",
    "    print p.text\n",
    "    article = p.text\n",
    "    f.write(article.encode('utf-8')+'\\n')\n",
    "\n",
    "#日期\n",
    "time = soup.select('.gggs')[0].select('time')[0].text\n",
    "#print time.text\n",
    "f.write(time.encode('utf-8')+'\\n')\n",
    "\n",
    "#人氣\n",
    "click = soup.select('#toolbar')[0].select('a')[3].text \n",
    "#print click.text\n",
    "f.write(click.encode('utf-8')+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟三、讀取url檔案並存取每個新聞需要的內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#在 readlines() 的時候其實會把隱藏的\\n讀進去，若檔案有經過調整，會變成檔案讀不到，要從跑上面的步驟一\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "count = 0\n",
    "f = open('C:/Users/BIG DATA/Desktop/Apple_news/20160310.text','r')\n",
    "for url in f.readlines():\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text)\n",
    "        file_date,file_num = url.split('/')[6],url.split('/')[7]  #替檔案取名子  \n",
    "        f = open('C:/Users/BIG DATA/Desktop/Apple_news/{}_{}.text'.format(file_date,file_num),'w')  \n",
    "   \n",
    "        #抓標題跟副標題\n",
    "        title1 = soup.select('#h1')[0].text\n",
    "        title2 = soup.select('#h2')[0].text            \n",
    "        f.write(title1.encode('utf-8')+'\\n')\n",
    "        f.write(title2.encode('utf-8')+'\\n')\n",
    "      \n",
    "        \n",
    "        #抓副標題，會出現問題，暫時不要爬      \n",
    "        #h2s = div.select('h2')            \n",
    "        #for h2 in h2s:\n",
    "            #sub_titile = h2.text\n",
    "            #f.write(sub_titile.encode('utf-8')+'\\n')\n",
    "        \n",
    "        #抓內文\n",
    "        div= soup.select('.articulum')[0]   #先把所有內文抓到\n",
    "        ps = div.select('p')                #再把裡面所有的p標籤抓到\n",
    "        for p in ps:                        #把每個p標籤一個一個抓出來印\n",
    "            article = p.text\n",
    "            f.write(article.encode('utf-8')+'\\n')\n",
    "        \n",
    "        #抓日期\n",
    "        time = soup.select('.gggs')[0].select('time')[0].text\n",
    "        f.write(time.encode('utf-8')+'\\n')\n",
    "        \n",
    "        #抓人氣\n",
    "        click = soup.select('#toolbar')[0].select('a')[3].text \n",
    "        f.write(click.encode('utf-8')+'\\n')\n",
    "        \n",
    "        #內文抓取檔案關閉\n",
    "        f.close()\n",
    "        \n",
    "    except IndexError:\n",
    "        count +=1\n",
    "        print count\n",
    "        continue\n",
    "#url 檔案關閉\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 步驟四、日期迴圈製造每一天的蘋果新聞連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.appledaily.com.tw/appledaily/archive/20160101\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160102\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160103\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160104\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160105\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160106\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160107\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160108\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160109\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160110\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160111\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160112\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160113\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160114\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160115\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160116\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160117\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160118\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160119\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160120\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160121\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160122\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160123\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160124\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160125\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160126\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160127\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160128\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160129\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160130\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160131\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160201\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160202\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160203\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160204\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160205\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160206\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160207\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160208\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160209\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160210\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160211\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160212\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160213\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160214\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160215\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160216\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160217\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160218\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160219\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160220\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160221\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160222\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160223\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160224\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160225\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160226\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160227\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160228\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160229\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160301\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160302\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160303\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160304\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160305\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160306\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160307\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160308\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160309\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160310\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160311\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160312\n"
     ]
    }
   ],
   "source": [
    "#製造日期的迴圈\n",
    "import datetime as dt\n",
    "startdate = dt.datetime(2016, 3,9)\n",
    "endate = dt.datetime(2016, 3,12)\n",
    "totaldays = (endate - startdate).days + 1\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\") \n",
    "    #print datestring\n",
    "    #每一天的蘋果新聞頁面\n",
    "    page_url = 'http://www.appledaily.com.tw/appledaily/archive/{}'.format(datestring)\n",
    "    print page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步驟五、抓每一天新聞的url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.appledaily.com.tw/appledaily/archive/20160310\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160311\n",
      "http://www.appledaily.com.tw/appledaily/archive/20160312\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urlparse import urljoin\n",
    "import os\n",
    "import datetime as dt\n",
    "#製造日期的迴圈\n",
    "startdate = dt.datetime(2016, 3,10)\n",
    "endate = dt.datetime(2016, 3,12)\n",
    "totaldays = (endate - startdate).days + 1\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\") \n",
    "    #print datestring\n",
    "    #每一天的蘋果新聞頁面\n",
    "    page_url = 'http://www.appledaily.com.tw/appledaily/archive/{}'.format(datestring)\n",
    "    print page_url\n",
    "    \n",
    "    news_url=[]\n",
    "    main_url ='http://www.appledaily.com.tw/appledaily'\n",
    "    res = requests.get(page_url)\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    #包含所有種類的div\n",
    "    div = soup.select('.abdominis.clearmen')[0]\n",
    "    #抓取所有連結\n",
    "    news_url=[]\n",
    "    lis = div.select('li')\n",
    "    for li in lis:\n",
    "        href = li.select('a')[0]['href']\n",
    "        #news_url.append(urljoin(main_url,'/'.join(href.split('/')[:6])))  #取的新聞連結完整網址\n",
    "        news_url.append(urljoin(main_url,href)) #urljoin會自動合併兩個網址\n",
    "        f = open('C:/Users/BIG DATA/Desktop/Apple_news/{}.text'.format(datestring),'w')\n",
    "        for i in news_url:\n",
    "            f.write(i.encode('utf-8')+'\\n') #prettify()先把檔案轉unicode，再用encode('utf-8')編碼\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 步驟六、讀取每一天的url 並把新聞內容抓下來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "#在 readlines() 的時候其實會把隱藏的\\n讀進去，若檔案有經過調整，會變成檔案讀不到，要從跑上面的步驟一\n",
    "import datetime as dt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "count = 0\n",
    "#製造日期的迴圈\n",
    "startdate = dt.datetime(2016, 3,10)\n",
    "endate = dt.datetime(2016, 3,12)\n",
    "totaldays = (endate - startdate).days + 1\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\") \n",
    "    #開起每天的url檔案\n",
    "    f = open('C:/Users/BIG DATA/Desktop/Apple_news/{}.text'.format(datestring),'r')\n",
    "    for url in f.readlines():\n",
    "        try:\n",
    "            res = requests.get(url)\n",
    "            soup = BeautifulSoup(res.text)\n",
    "            file_date,file_num = url.split('/')[6],url.split('/')[7]  #替檔案取名子  \n",
    "            f = open('C:/Users/BIG DATA/Desktop/Apple_news/{}_{}.text'.format(file_date,file_num),'w')  \n",
    "\n",
    "            #抓標題跟副標題\n",
    "            title1 = soup.select('#h1')[0].text\n",
    "            title2 = soup.select('#h2')[0].text            \n",
    "            f.write(title1.encode('utf-8')+'\\n')\n",
    "            f.write(title2.encode('utf-8')+'\\n')\n",
    "\n",
    "\n",
    "            #抓副標題，會出現問題，暫時不要爬      \n",
    "            #h2s = div.select('h2')            \n",
    "            #for h2 in h2s:\n",
    "                #sub_titile = h2.text\n",
    "                #f.write(sub_titile.encode('utf-8')+'\\n')\n",
    "\n",
    "            #抓內文\n",
    "            div= soup.select('.articulum')[0]   #先把所有內文抓到\n",
    "            ps = div.select('p')                #再把裡面所有的p標籤抓到\n",
    "            for p in ps:                        #把每個p標籤一個一個抓出來印\n",
    "                article = p.text\n",
    "                f.write(article.encode('utf-8')+'\\n')\n",
    "\n",
    "            #抓日期\n",
    "            time = soup.select('.gggs')[0].select('time')[0].text\n",
    "            f.write(time.encode('utf-8')+'\\n')\n",
    "\n",
    "            #抓人氣\n",
    "            click = soup.select('#toolbar')[0].select('a')[3].text \n",
    "            f.write(click.encode('utf-8')+'\\n')\n",
    "\n",
    "            #內文抓取檔案關閉\n",
    "            f.close()\n",
    "\n",
    "        except IndexError:\n",
    "            count +=1\n",
    "            print count\n",
    "            continue\n",
    "    #url 檔案關閉\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
